from datetime import datetime, timedelta

#import airflow.hooks.S3_hook
from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.utils.dates import days_ago

from model import train_model

# data source: data_s3_path = ' https://iasd-data-in-the-cloud.s3.eu-west-3.amazonaws.com/petrol_consumption.csv'
# the data was copied into my own S3 bucket
# set up some variables
remote_bucket = 'iasd-klouvi-data'  # S3 bucket name where to store data and trained models
data_path = 'petrol_consumption.csv'  # dataset file name
trained_model_path = 'petrol_consumption_model.pickle'  # file where to save the trained model
aws_credentials_key = 'aws_credentials'

args = {
    'owner': 'rouxel',  # the owner id
    'start_date': days_ago(0),
    'retry_delay': timedelta(seconds=10),
}

dag = DAG('iris',
    default_args=args,
    schedule_interval="0 * * * *")


def download_from_s3_task(aws_credentials, bucket_name, key, output_path, **kwargs):
    """
        Download data from S3 bucket
    """
    hook = airflow.hooks.S3_hook.S3Hook(aws_credentials)  # AWS credentials are stored into Airflow connections manager
    source_object = hook.get_key(key, bucket_name)
    source_object.download_file(output_path)



def upload_to_s3_task(aws_credentials, bucket_name, filename, **kwargs):
    """
        Upload data into S3 bucket
    """
    hook = airflow.hooks.S3_hook.S3Hook(aws_credentials)
    key = datetime.now().strftime(
        "%Y/%m/%d/%H/") + filename  # the trained model is pushed into S3 under the folder YYYY/MM/dd/HH
    hook.load_file(filename, key, bucket_name)


# let's start with a dummy task
start_task = DummyOperator(
    task_id='start_task',
    dag=dag,
)

# task to download the dataset from s3
download_data_from_s3_task = PythonOperator(
    task_id='download_data_from_s3_task',
    provide_context=True,
    python_callable=download_from_s3_task,
    op_kwargs={'aws_credentials': aws_credentials_key, 'bucket_name': remote_bucket, 'key': data_path,
               'output_path': data_path},
    dag=dag,
)

# task to train the model (see model.py)
train_model_task = PythonOperator(
    task_id='train_model_task',
    provide_context=True,
    python_callable=train_model,
    op_kwargs={'dataset_filepath': data_path, 'trained_model_path': trained_model_path},
    dag=dag,
)

# task to upload the trained model to s3 (yours)
upload_model_to_s3_task = PythonOperator(
    task_id='upload_to_s3_task',
    provide_context=True,
    python_callable=upload_to_s3_task,
    op_kwargs={'aws_credentials': aws_credentials_key, 'bucket_name': remote_bucket, 'filename': trained_model_path},
    dag=dag,
)

# just a dummy task to end the DAGs
end_task = DummyOperator(
    task_id='end_task',
    dag=dag,
)

# chain the pipeline start -> download -> train -> upload -> end
start_task >> download_data_from_s3_task >> train_model_task >> upload_model_to_s3_task >> end_task





import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import pickle


# This filepath should be passed as an argument
def train_model(dataset_filepath, trained_model_path, **kwargs):
    dataset = pd.read_csv(dataset_filepath)

    attributes = dataset.iloc[:, 0:4].values
    labels = dataset.iloc[:, 4].values

    # Divide the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        attributes, labels, test_size=0.2, random_state=0
    )

    # Feature Scaling
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)

    # Train the model
    regressor = RandomForestRegressor(n_estimators=200, random_state=0)
    regressor.fit(X_train, y_train)
    y_pred = regressor.predict(X_test)

    # Evaluate the performance of the model